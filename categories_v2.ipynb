{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: background\n",
      "1: peanut butter\n",
      "2: ice cream\n",
      "3: chocolate chip\n",
      "4: sweet potato\n",
      "5: crock pot\n",
      "6: pork chops\n",
      "7: cream cheese\n",
      "8: black bean\n",
      "9: potato salad\n",
      "10: chicken salad\n",
      "11: sour cream\n",
      "12: goat cheese\n",
      "13: pasta salad\n",
      "14: salad recipe\n",
      "15: green beans\n",
      "16: chip cookies\n",
      "17: blue cheese\n",
      "18: french toast\n",
      "19: butternut squash\n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:/Users/Elwin/Desktop/Data Science & Analytics/Year 3/CS3244/Project/Data'\n",
    "keys_path = 'C:/Users/Elwin/Desktop/Data Science & Analytics/Year 3/CS3244/Project/Data/val_keys.pkl'\n",
    "name_path = 'C:/Users/Elwin/Desktop/Data Science & Analytics/Year 3/CS3244/Project/Data/classes1M.pkl'\n",
    "\n",
    "env = lmdb.open(os.path.join(data_path, 'val_lmdb'), max_readers=1, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "\n",
    "ids = pickle.load(open(keys_path, 'rb'))\n",
    "f = open(name_path, 'rb')\n",
    "_ = pickle.load(f)\n",
    "categories = pickle.load(f)\n",
    "\n",
    "for i in range(20):\n",
    "  print(\"{}: {}\".format(i, categories[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = []\n",
    "classes = []\n",
    "def path_for(filename):\n",
    "  return os.path.join(images_path, filename[0], filename[1], filename[2], filename[3], filename)\n",
    "\n",
    "for id in ids:\n",
    "  with env.begin(write=False) as txn:\n",
    "    byte_sample = txn.get(id.encode())\n",
    "  sample = pickle.loads(byte_sample, encoding='latin')\n",
    "  sample.pop('imgs',None)\n",
    "  sample['intrs'] = np.average(sample['intrs'], axis=0)\n",
    "  #sample['intrs'] = np.sum(sample['intrs'], axis=0) #I used average instead as it is more recommended on the web\n",
    "  entry = np.concatenate([sample['ingrs'],sample['intrs']])\n",
    "  classes.append(sample['classes'])\n",
    "  storage.append(entry.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51129, 1044)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(storage)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df)\n",
    "df = pd.DataFrame(scaler.transform(df).tolist())\n",
    "df[\"classes\"] = classes\n",
    "dummies = pd.get_dummies(df[\"classes\"])\n",
    "dummies.columns = ['class' + str(col) for col in dummies.columns]\n",
    "df = pd.concat([df, dummies], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the the train and test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, random_state=42, test_size=0.30, shuffle=True)\n",
    "x_train = train.iloc[:, 0:1043]\n",
    "y_train = train.iloc[:,1045:]\n",
    "x_test = test.iloc[:, 0:1043]\n",
    "y_test = test.iloc[:,1045:]\n",
    "allcategories = list(y_test.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OnevsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing class1 comments...**\n",
      "[1 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "multilabels = pd.DataFrame()\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in allcategories:\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    multilabels[category] = prediction #This is the multilabel dataset\n",
    "    #print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    #print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
